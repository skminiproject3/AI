import os
import re
import hashlib
import json
import logging
from typing import List, Tuple, Optional, Dict, Any
#from aiohttp import request
from fastapi import FastAPI, UploadFile, File, HTTPException #, Form,Body
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_tavily import TavilySearch
import pymysql
# ==========================================================
# í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", "3306"))
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY ë¯¸ì„¤ì •!")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
tavily_tool = TavilySearch(api_key=TAVILY_API_KEY, max_results=3) if TAVILY_API_KEY else None

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

UPLOAD_DIR = "uploaded_pdfs"
VECTOR_DIR = "vector_stores"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(VECTOR_DIR, exist_ok=True)

# ==========================================================
# Pydantic ëª¨ë¸ ì •ì˜
# ==========================================================
class PdfPathsRequest(BaseModel):
    pdf_paths: List[str]

class ChapterRequest(BaseModel):
    pdf_paths: Optional[List[str]] = None
    chapter: int
    #chapter_request: Optional[str] = Field(None, alias="chapterRequest")
class ChapterSummary(BaseModel):
    chapter: str
    summaryText: str

class SummariesResponse(BaseModel):
    summaries: List[ChapterSummary]
class QuestionRequest(BaseModel):
    question: str
    force_web: bool
    pdf_paths: Optional[List[str]] = None

class QuizGenerationRequest(BaseModel):
    pdf_paths: List[str]
    num_questions: int = Field(5, ge=1, le=20)
    difficulty: str = Field("MEDIUM", pattern="^(EASY|MEDIUM|HARD)$")


def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        return "\n".join([p.page_content for p in pages])
    except Exception as e:
        logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path} | {e}")
        return ""
    
# ==========================================================
# âœ… 3ï¸âƒ£ ë²¡í„° DBì—ì„œ ì±•í„°ë³„ ë‚´ìš© ì¶”ì¶œ
# ==========================================================
def load_chapter_text_from_vector(vector_path: str, target_chapter: str) -> str:
    vs = FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)
    
    combined_text = ""
    for doc in vs.docstore._dict.values():
        combined_text += getattr(doc, "page_content", "") + "\n"

    # metadata.json ìˆœì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì› ìœ„ì¹˜ ì°¾ê¸°
    metadata_path = os.path.join(vector_path, "metadata.json")
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    chapters = sorted(metadata.get("chapters", []), key=lambda x: [int(n) for n in x["chapter"].split(".")])
        # íƒ€ê²Ÿ ì±•í„° ë©”íƒ€ë°ì´í„° ì°¾ê¸°
    target_meta = next((ch for ch in chapters if ch.get("chapter") == target_chapter), None)
    
    if not target_meta:
        raise HTTPException(status_code=404, detail=f"metadata.jsonì— {target_chapter} ì—†ìŒ")
    target_title = target_meta.get("title", "").strip()
    WINDOW = 10  # ì•ë’¤ 20ê¸€ì ë‚´ì—ì„œ title í™•ì¸

    # --- ì‹œì‘ ìœ„ì¹˜ íƒìƒ‰ (ì´ì¤‘ ì¡°ê±´: ë²ˆí˜¸ + title proximity)
    start_idx = -1
    for m in re.finditer(re.escape(target_chapter), combined_text):
        idx = m.start()
        # ì£¼ë³€ 20ê¸€ì ë‚´ í™•ì¸
        context_start = max(0, idx - WINDOW)
        context_end = min(len(combined_text), idx + WINDOW)
        context = combined_text[context_start:context_end]
        if target_title and target_title in context:
            start_idx = idx
            break

    if start_idx == -1:
        raise HTTPException(status_code=404, detail=f"{target_chapter} ì‹œì‘ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë²ˆí˜¸ì™€ ì œëª©ì´ ê·¼ì ‘í•˜ì§€ ì•ŠìŒ)")

    # ë‹¤ìŒ ë‹¨ì› ì‹œì‘ ìœ„ì¹˜
    next_idx = len(combined_text)
    for ch in chapters:
        if ch["chapter"] > target_chapter:
            idx = combined_text.find(ch["chapter"])
            if idx != -1:
                next_idx = idx
                break
    print(f"ë‹¨ì› í…ìŠ¤íŠ¸ ì¶”ì¶œ: {target_chapter} | start={start_idx} | end={next_idx}")
    
    return combined_text[start_idx:next_idx]


# ==========================================================
# âœ… 4ï¸âƒ£ LLMì„ ì´ìš©í•´ ìš”ì•½ ìˆ˜í–‰
# ==========================================================
def summarize_text_with_llm(chapter_label: str, text: str) -> str:
    """
    LangChain + OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ìš”ì•½
    """
    prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜:\n\n{text[:12000]}"
    result = llm.invoke(prompt)
    return result.content.strip()

def get_or_create_vectorstore(pdf_path: str) -> Optional[FAISS]:
    normalized_path = pdf_path.replace("\\","/")
    path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
    vector_path = os.path.join(VECTOR_DIR, path_hash)
    metadata_path = os.path.join(vector_path, "metadata.json")

    if os.path.exists(vector_path) and os.path.exists(metadata_path):
        logger.info(f"ğŸ“‚ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë° metadata.json ë¡œë“œ: {pdf_path}")
        return FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)

    # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = extract_text_from_pdf(pdf_path)
    chapters = recognize_chapters_with_llm(texts)

    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vs = FAISS.from_texts([texts], embeddings)
    os.makedirs(vector_path, exist_ok=True)
    vs.save_local(vector_path)

    # metadata.json ì €ì¥
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump({"chapters": chapters}, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ + metadata.json ìƒì„± ì™„ë£Œ: {pdf_path}")

    return vs

def combine_vectorstores(pdf_paths: List[str]) -> Tuple[Optional[FAISS], Optional[str]]:
    vectorstores = []
    combined_content = ""
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            continue
        vs = get_or_create_vectorstore(pdf_path)
        if vs:
            vectorstores.append(vs)
            combined_content += extract_text_from_pdf(pdf_path) + "\n\n--- PDF ë¶„ë¦¬ ---\n\n"
    if not vectorstores:
        return None, None
    main_vs = vectorstores[0]
    for i in range(1, len(vectorstores)):
        main_vs.merge_from(vectorstores[i])
    return main_vs, combined_content.strip()
# -----------------------
# ì±•í„° ì¸ì‹
# -----------------------
def recognize_chapters_with_llm(full_text: str):
    """
    ì²« ë“±ì¥ ìˆ«ì ê¸°ë°˜ ì¥ ì¸ì‹ í›„ LLMìœ¼ë¡œ ê²€ì¦
    """
    chapters = []
    first_match = re.search(r"^(\d+)\s", full_text, re.MULTILINE)
    if first_match:
        chapters.append({"chapter": first_match.group(1), "title": ""})

    # LLMìœ¼ë¡œ ë³´ì •
    try:
        prompt = f"""
ì•„ë˜ëŠ” PDF í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ì—ì„œ 'ì¥ ë²ˆí˜¸'ì™€ 'ì¥ ì œëª©'ì„ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.
ì˜ˆ: {{ "chapters": [{{"chapter": "1", "title": "ì•”í˜¸ ê°œë¡ "}}] }}

í…ìŠ¤íŠ¸:
{full_text[:10000]}
"""
        resp = llm.invoke(prompt)
        json_text = resp.content.strip().replace("```json","").replace("```","")
        llm_chapters = json.loads(json_text).get("chapters", [])
        if llm_chapters:
            chapters = llm_chapters
    except Exception as e:
        logger.warning(f"LLM ì±•í„° ì¸ì‹ ì‹¤íŒ¨: {e}")

    return chapters

# ==========================================================
# ì±•í„° ê°ì§€ ë¡œì§
# ==========================================================
def detect_chapters_by_regex(text: str) -> List[str]:
    """
    PDF í…ìŠ¤íŠ¸ì—ì„œ ì±•í„° ë²ˆí˜¸ë¥¼ ì¶”ì¶œ
    - ë²ˆí˜¸ í˜•ì‹: 1.1, 2.3.4, ì œ1ì¥, CHAPTER 1 ë“±
    - ì¤‘ë³µ ì œê±° í›„ ì •ë ¬
    """
    try:
        chapters = set()
        # 1) ì°¨ë¡€ì²˜ëŸ¼ ë³´ì´ëŠ” ë²ˆí˜¸ ìš°ì„  ì¶”ì¶œ
        toc_matches = re.findall(r"\b\d+(?:\.\d+)+\b", text)
        if toc_matches:
            return sorted(toc_matches, key=lambda x: [int(n) for n in x.split('.')])
        
        # 2) ê° ì¤„ ìŠ¤ìº”
        pattern = re.compile(r"(\b\d+(?:\.\d+)+\b|ì œ\d+ì¥|CHAPTER\s+\d+)", re.IGNORECASE)
        for line in text.splitlines():
            line = line.strip()
            match = pattern.search(line)
            if match:
                chapters.add(match.group(1))
        
        # ì •ë ¬: ìˆ«ì ê¸°ë°˜ ìš°ì„ , ë¬¸ì í¬í•¨ ì±•í„°ëŠ” ë’¤ë¡œ
        def sort_key(ch):
            nums = re.findall(r"\d+", ch)
            return [int(n) for n in nums] if nums else [float('inf')]
        
        return sorted(chapters, key=sort_key)
    except Exception as e:
        logger.error(f"ì±•í„° ê°ì§€ ì‹¤íŒ¨ | {e}")
        return []

def detect_chapters_by_llm(text: str) -> List[str]:
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì±•í„° ì œëª©ì„ ìˆœì„œëŒ€ë¡œ ë½‘ì•„ì£¼ì„¸ìš”.
ì˜ˆì‹œ ì¶œë ¥: ["ì œ1ì¥ ê°œìš”", "ì œ2ì¥ ì´ë¡ "]
---
{text[:4000]}
"""
    try:
        response = llm.invoke(prompt)
        lines = re.findall(r"ì œ?\s*\d+\s*ì¥|CHAPTER\s+\d+|Chapter\s+\d+|\d+\.\d+", response.content)
        return sorted(set(lines))
    except Exception as e:
        logger.warning(f"âš ï¸ LLM ì±•í„° ê°ì§€ ì‹¤íŒ¨: {e}")
        return []

def get_total_chapters(text: str) -> Dict[str, Any]:
    regex_chapters = detect_chapters_by_regex(text)
    if regex_chapters:
        return {"method": "regex", "total_chapters": len(regex_chapters), "chapter_list": regex_chapters}
    llm_chapters = detect_chapters_by_llm(text)
    if llm_chapters:
        return {"method": "llm", "total_chapters": len(llm_chapters), "chapter_list": llm_chapters}
    return {"method": "none", "total_chapters": 1, "chapter_list": []}

# ==========================================================
# DBì—ì„œ content_idë¡œ PDF ê²½ë¡œ ì¡°íšŒ
# ==========================================================
def get_pdf_paths_for_content(content_id: int):
    connection = None
    paths = []
    try:
        connection = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_DATABASE,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )
        with connection.cursor() as cursor:
            sql = "SELECT file_path FROM contents WHERE id=%s"
            cursor.execute(sql, (content_id,))
            result = cursor.fetchall()
            paths = [row['file_path'] for row in result if row['file_path']]
    except Exception as e:
        logger.error(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if connection:
            connection.close()
    return paths
def get_vector_paths_for_content(content_id: int) -> List[str]:
    """
    content_id ê¸°ì¤€ìœ¼ë¡œ MariaDBì—ì„œ vector_pathë¥¼ ì¡°íšŒ
    ë°˜í™˜: ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    connection = None
    paths = []

    try:
        connection = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_DATABASE,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        with connection.cursor() as cursor:
            sql = "SELECT vector_path FROM contents WHERE id=%s"
            cursor.execute(sql, (content_id,))
            result = cursor.fetchall()
            paths = [row['vector_path'] for row in result if row['vector_path']]

    except Exception as e:
        print(f"âŒ ë²¡í„° ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if connection:
            connection.close()

    return paths
# ==========================================================
# FastAPI ì´ˆê¸°í™”
# ==========================================================
app = FastAPI(title="PDF í•™ìŠµ ë„ìš°ë¯¸ API (Full í†µí•©)")

# ==========================================================
# âœ… /health
# ==========================================================
@app.get("/health/")
async def health_check():
    return {"status": "ok"}

# ==========================================================
# âœ… /upload_pdfs
# ==========================================================
@app.post("/upload_pdfs/")
async def upload_pdfs(files: List[UploadFile] = File(...)):
    if not files:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    logger.info("ğŸ“¥ ì—…ë¡œë“œ ìš”ì²­ ìˆ˜ì‹  | files=%d", len(files))

    saved_files: List[str] = []
    combined_text_parts: List[str] = []

    for file in files:
        save_path = os.path.join(UPLOAD_DIR, file.filename)
        try:
            # íŒŒì¼ ì €ì¥
            with open(save_path, "wb") as f:
                data = await file.read()
                f.write(data)
            saved_files.append(save_path)
            logger.info("ğŸ—‚ï¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ | path=%s | size=%d bytes", save_path, len(data))

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = extract_text_from_pdf(save_path)
            if text.strip():
                combined_text_parts.append(text)
            else:
                logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ë¬¸ì„œ | path=%s", save_path)

        except Exception as e:
            logger.error("âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ | file=%s | error=%s", file.filename, e)
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    if not combined_text_parts:
        raise HTTPException(status_code=400, detail="PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ì „ì²´ ë¬¸ì„œ ê¸°ì¤€ ì±•í„° ê°ì§€
    combined_text = "\n\n--- PDF ë¶„ë¦¬ ---\n\n".join(combined_text_parts)
    detection = get_total_chapters(combined_text)

    total_chapters_result: int = int(detection.get("total_chapters", 1))
    detected_method: str = str(detection.get("method", "none"))
    all_chapters: List[str] = list(detection.get("chapter_list", []))

    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° vector_path ê³„ì‚° (ë¬´ì¡°ê±´ ìƒì„±)
    vector_paths: List[str] = []
    for file_path in saved_files:
        vs = get_or_create_vectorstore(file_path)
        normalized_path = file_path.replace("\\", "/")
        path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
        vector_path = os.path.join(VECTOR_DIR, path_hash)
        vector_paths.append(vector_path)

    # ê¸°ë³¸ì ìœ¼ë¡œ í•˜ë‚˜ë§Œ ë°˜í™˜ (boot ì—°ë™ìš©)
    final_vector_path = vector_paths[0] if vector_paths else None
    logger.info(
        "âœ… ì—…ë¡œë“œ/ë¶„ì„ ì™„ë£Œ | files=%d | total_chapters=%d | method=%s | vector_path=%s",
        len(saved_files), total_chapters_result, detected_method, final_vector_path
    )

    return {
        "pdf_paths": saved_files,
        "total_chapters": total_chapters_result,
        "method": detected_method,
        "chapter_list": all_chapters,
        "vector_path": final_vector_path
    }
# ==========================================================
# âœ… /summarize (ì „ì²´ ìš”ì•½)
# ==========================================================
@app.post("/api/contents/{content_id}/summarize")
async def summarize_full(content_id: int):
    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")
    _, combined_content = combine_vectorstores(pdf_paths)
    if not combined_content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨")
    prompt = f"""
ë‹¤ìŒ PDF ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
í•µì‹¬ ìœ„ì£¼, 5ë¬¸ë‹¨ ì´í•˜.
---
{combined_content[:10000]}
"""
    result = llm.invoke(prompt)
    return {"content_id": content_id, "summaryText": result.content.strip()}

# ==========================================================
# âœ… /summaries (ë‹¨ì›ë³„ ìš”ì•½)
# ==========================================================
@app.post("/api/contents/{content_id}/summaries")
def summarize_by_chapter(content_id: int, request: ChapterRequest):
    
    if not request.chapter:
        raise HTTPException(status_code=400, detail="chapter í•„ìˆ˜")

    # 1ï¸âƒ£ content_id ê¸°ì¤€ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì¡°íšŒ
    vector_paths = get_vector_paths_for_content(content_id)
    if not vector_paths:
        raise HTTPException(status_code=404, detail="Vector path not found")

    vector_path = vector_paths[0]
    
    # 2ï¸âƒ£ metadata.json ë¡œë“œ
    metadata_path = os.path.join(vector_path, "metadata.json")
    if not os.path.exists(metadata_path):
        raise HTTPException(status_code=404, detail="metadata.json ì—†ìŒ")

    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)
        
    chapters = metadata.get("chapters", [])
    if not chapters:
        raise HTTPException(status_code=404, detail="metadata.jsonì— ë‹¨ì›ì´ ì—†ìŒ")
    
    # 3ï¸âƒ£ ë§ˆì§€ë§‰ ìˆ«ì ê¸°ì¤€ chapter ì°¾ê¸°
    target_chapter_obj = None
    for ch in chapters:
        parts = ch["chapter"].split(".")
        if parts[-1] == str(request.chapter):
            target_chapter_obj = ch
            
            break

    if not target_chapter_obj:
            return {
                "summaries": [
                    {
                        "chapter": str(request.chapter),
                        "title": "",
                        "summaryText": f" ìš”ì²­ í•˜ì‹  chapterëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    }
                ]
            }
    
    target_chapter = target_chapter_obj["chapter"]
    chapter_title = target_chapter_obj.get("title", "")
    
    # 4ï¸âƒ£ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ í•´ë‹¹ chapter í…ìŠ¤íŠ¸ ì¶”ì¶œ
    chapter_text = load_chapter_text_from_vector(vector_path, target_chapter)

    # 5ï¸âƒ£ LLMìœ¼ë¡œ ìš”ì•½
    summary_text = summarize_text_with_llm(target_chapter, chapter_text)

    # 6ï¸âƒ£ ê²°ê³¼ ë°˜í™˜
    return {
        "summaries": [
            {
                "chapter": target_chapter,
                "title": chapter_title,
                "summaryText": summary_text
            }
        ]
    }


# -----------------------
# LangChain Agent ê¸°ë°˜ ì§ˆë¬¸ ì—”ë“œí¬ì¸íŠ¸
# -----------------------
@app.post("/api/contents/{content_id}/ask")
async def ask_question(content_id: int, body: QuestionRequest):
    """
    - content_id: Boot DBì˜ contents.id
    - question: JSON bodyë¡œ ì „ì†¡ {"question": "...", "force_web": false}
    - force_web: (ì˜µì…˜) Trueë¡œ ë³´ë‚´ë©´ PDF í™•ì¸ ì—†ì´ ë°”ë¡œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    """
    question = body.question
    force_web = body.force_web

    logger.info(f"ì§ˆë¬¸ ìˆ˜ì‹ : content_id={content_id} / force_web={force_web} / question={question}")

    # DBì—ì„œ PDF ê²½ë¡œ ì¡°íšŒ
    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        logger.warning("PDF ê²½ë¡œ ì—†ìŒ: ì›¹ í´ë°± ì‹œë„")
        if not tavily_tool:
            raise HTTPException(status_code=404, detail="PDF ë° ì›¹ ê²€ìƒ‰ ë¶ˆê°€ (íŒŒì¼ ì—†ìŒ, Tavily ë¯¸ì„¤ì •)")
        pdf_paths = []

    # ectorstore ì¤€ë¹„
    vectorstore = None
    combined_text = None
    if pdf_paths:
        vectorstore, combined_text = combine_vectorstores(pdf_paths)
        if not vectorstore:
            logger.warning("Vectorstore ë¡œë“œ ì‹¤íŒ¨ â†’ ì›¹ í´ë°±")

    # force_web ë˜ëŠ” â€œì›¹ì—ì„œ/ê²€ìƒ‰â€ í‚¤ì›Œë“œ ìë™ ê°ì§€
    autotrig_web_keywords = any(k in question for k in ["ì›¹ì—ì„œ", "ì¸í„°ë„·", "ê²€ìƒ‰", "ì›¹ìœ¼ë¡œ"])
    do_force_web = force_web or autotrig_web_keywords

    # PDF RAG ì‹œë„ (force_web=False ì¼ ë•Œë§Œ)
    if vectorstore and not do_force_web:
        try:
            retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
            rag_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=retriever,
                return_source_documents=True
            )

            pdf_response = rag_chain.invoke({"query": question})
            logger.debug(f"PDF RAG ì‘ë‹µ: keys={list(pdf_response.keys()) if isinstance(pdf_response, dict) else 'N/A'}")

            # PDF ê²°ê³¼ ì •ë¦¬
            result_text = ""
            sources = []
            if isinstance(pdf_response, dict):
                result_text = str(pdf_response.get("result") or pdf_response.get("output_text") or "")
                sources = pdf_response.get("source_documents") or []
            else:
                result_text = str(pdf_response)

            # ê²°ê³¼ ê²€ì¦
            if sources and len(result_text.strip()) > 10:
                refs = []
                for s in sources[:3]:
                    try:
                        meta = getattr(s, "metadata", {}) or (s.get("metadata") if isinstance(s, dict) else {})
                        snippet = (getattr(s, "page_content", "") or s.get("page_content", ""))[:300]
                        refs.append({"metadata": meta, "snippet": snippet})
                    except Exception:
                        refs.append({"raw": str(s)[:300]})
                return {
                    "source": "PDF",
                    "content_id": content_id,
                    "question": question,
                    "answer": result_text.strip(),
                    "pdf_reference_count": len(sources),
                    "pdf_references": refs,
                    "message": "PDF ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                }
            else:
                logger.info("PDFì—ì„œ ìœ ì˜ë¯¸í•œ ê·¼ê±° ëª»ì°¾ìŒ â†’ ì›¹ í´ë°±")
        except Exception as e:
            logger.exception(f"PDF RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ì‹œ ì›¹ê²€ìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°

    # ì›¹ í´ë°± (Tavily ê²€ìƒ‰)
    if tavily_tool:
        try:
            query_clean = re.sub(r"(ì›¹ì—ì„œ|ì›¹ìœ¼ë¡œ|ì¸í„°ë„·|ê²€ìƒ‰|í•´ì¤˜|ì•Œë ¤ì¤˜)", "", question, flags=re.IGNORECASE).strip()
            if not query_clean:
                query_clean = question

            web_resp = tavily_tool.invoke({"query": query_clean})
            results = web_resp.get("results", []) if isinstance(web_resp, dict) else web_resp

            if results:
                sorted_results = sorted(results, key=lambda x: x.get("score", 0), reverse=True)
                best = sorted_results[0]
                title = best.get("title") or best.get("url") or "ì¶œì²˜ ì—†ìŒ"
                url = best.get("url", "")
                extracted = best.get("content") or best.get("snippet") or ""

                if extracted and len(extracted) > 30:
                    prompt = f"""
ì•„ë˜ ì¶œì²˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•˜ê³  ì •í™•í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
ì¶œì²˜ ì œëª©: {title}
ì¶œì²˜ URL: {url}
ì¶œì²˜ ë‚´ìš©:
{extracted}

ì§ˆë¬¸: {question}

- ë‹µë³€ì€ 3~6 ë¬¸ì¥ ì´ë‚´ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ì¶œì²˜ URLì„ 'reference' í•„ë“œì— í¬í•¨ì‹œí‚¤ì„¸ìš”.
"""
                    summary = llm.invoke(prompt).content.strip()
                else:
                    summary = extracted or str(best)[:400]

                return {
                    "source": "WEB",
                    "content_id": content_id,
                    "question": question,
                    "answer": summary,
                    "reference": {"title": title, "url": url},
                    "message": "ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤."
                }

            logger.info("ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        except Exception as e:
            logger.exception(f"ì›¹ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨ ì‹œ
    return {
        "source": "NONE",
        "content_id": content_id,
        "question": question,
        "answer": "ê´€ë ¨ ì •ë³´ë¥¼ PDF ë° ì›¹ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "message": "ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ì™¸ë¶€ ì˜ì¡´ì„± ë¯¸ì„¤ì •"
    }

# -----------------------
# í€´ì¦ˆ ìƒì„±
# -----------------------
@app.post("/api/contents/{content_id}/quiz/generate")
async def quiz_generate(content_id: int, request: dict):
    try:
        # Bootì—ì„œ ì „ë‹¬í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        num_questions = request.get("num_questions")
        difficulty = request.get("difficulty")

        if not num_questions or not difficulty:
            raise HTTPException(status_code=400, detail="num_questions, difficulty í•„ìˆ˜")

        # âœ… Boot-DBì—ì„œ PDF ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        saved_pdfs = get_pdf_paths_for_content(content_id)
        if not saved_pdfs:
            raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")

        # âœ… PDFì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
        vectorstore, content = combine_vectorstores(saved_pdfs)
        if not content:
            raise HTTPException(status_code=400, detail="PDF ë¡œë“œ ì‹¤íŒ¨")

        # âœ… LLM ìš”ì²­ Prompt
        prompt = f"""
ë‹¹ì‹ ì€ ì‹œí—˜ ë¬¸ì œë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ í€´ì¦ˆ {num_questions}ê°œ ìƒì„±í•˜ì„¸ìš”.
ë‚œì´ë„: {difficulty}

ì¶œë ¥ í˜•ì‹(JSON):
{{
"questions":[
    {{
    "question": "ë¬¸ì œ",
    "options": ["A", "B", "C", "D"],
    "correct_answer": "ì •ë‹µ",
    "explanation": "ì •ë‹µ í•´ì„¤"
    }}
]}}
ë‚´ìš©:
{content[:12000]}
"""

        result = llm.invoke(prompt)

        try:
            cleaned = result.content.strip().replace("```json","").replace("```","")
            data = json.loads(cleaned)
            return data  # âœ… Bootê°€ ì›í•˜ëŠ” êµ¬ì¡° ê·¸ëŒ€ë¡œ ë°˜í™˜

        except Exception as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ì›ë³¸ ë°˜í™˜: {e}")
            return {"raw_response": result.content.strip()}

    except Exception as e:
        logger.error(f"í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))