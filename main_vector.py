import os
import re
import hashlib
import json
import logging
from typing import List, Tuple, Optional, Dict, Any
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_tavily import TavilySearch
import pymysql

# ==========================================================
# ğŸŒ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì´ˆê¸°í™”
# ==========================================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", "3306"))
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY ë¯¸ì„¤ì •!")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
tavily_tool = TavilySearch(api_key=TAVILY_API_KEY, max_results=3) if TAVILY_API_KEY else None

# ==========================================================
# ğŸ§© FastAPI ì´ˆê¸°í™” ë° CORS ì„¤ì •
# ==========================================================
app = FastAPI(title="PDF í•™ìŠµ ë„ìš°ë¯¸ API (Full í†µí•©)")

origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================================
# ğŸ—‚ï¸ ë””ë ‰í† ë¦¬ ìë™ ì„¤ì • (ë¡œì»¬ / ë°°í¬ ë‘˜ ë‹¤ ì§€ì›)
# ==========================================================
if os.getenv("ENV_MODE", "local") == "prod":
    UPLOAD_DIR = "/uploads"  # Docker í™˜ê²½
else:
    UPLOAD_DIR = os.path.join(os.getcwd(), "uploaded_pdfs")

VECTOR_DIR = "vector_stores"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(VECTOR_DIR, exist_ok=True)

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# ==========================================================
# ğŸ“„ Pydantic ëª¨ë¸ ì •ì˜
# ==========================================================
class PdfPathsRequest(BaseModel):
    pdf_paths: List[str]

class ChapterRequest(BaseModel):
    pdf_paths: Optional[List[str]] = None
    chapter_request: Optional[str] = None

class QuestionRequest(BaseModel):
    question: str
    force_web: bool
    pdf_paths: Optional[List[str]] = None

class QuizGenerationRequest(BaseModel):
    pdf_paths: List[str]
    num_questions: int = Field(5, ge=1, le=20)
    difficulty: str = Field("MEDIUM", pattern="^(EASY|MEDIUM|HARD)$")

# ==========================================================
# ğŸ”§ PDF ì²˜ë¦¬ ìœ í‹¸
# ==========================================================
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        return "\n".join([p.page_content for p in pages])
    except Exception as e:
        logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path} | {e}")
        return ""

def get_or_create_vectorstore(pdf_path: str) -> Optional[FAISS]:
    normalized_path = pdf_path.replace("\\", "/")
    path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
    vector_path = os.path.join(VECTOR_DIR, path_hash)
    if os.path.exists(vector_path):
        return FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)
    try:
        loader = PyPDFLoader(pdf_path)
        texts = [p.page_content for p in loader.load()]
        vs = FAISS.from_texts(texts, embeddings)
        vs.save_local(vector_path)
        logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {pdf_path}")
        return vs
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ìƒì„± ì‹¤íŒ¨ ({pdf_path}): {e}")
        return None

def combine_vectorstores(pdf_paths: List[str]) -> Tuple[Optional[FAISS], Optional[str]]:
    vectorstores = []
    combined_content = ""
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            continue
        vs = get_or_create_vectorstore(pdf_path)
        if vs:
            vectorstores.append(vs)
            combined_content += extract_text_from_pdf(pdf_path) + "\n\n--- PDF ë¶„ë¦¬ ---\n\n"
    if not vectorstores:
        return None, None
    main_vs = vectorstores[0]
    for i in range(1, len(vectorstores)):
        main_vs.merge_from(vectorstores[i])
    return main_vs, combined_content.strip()

# ==========================================================
# ğŸ§­ DBì—ì„œ PDF ê²½ë¡œ ì¡°íšŒ
# ==========================================================
def get_pdf_paths_for_content(content_id: int):
    connection = None
    paths = []
    try:
        connection = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_DATABASE,
            charset="utf8mb4",
            cursorclass=pymysql.cursors.DictCursor,
        )
        with connection.cursor() as cursor:
            sql = "SELECT file_path FROM contents WHERE id=%s"
            cursor.execute(sql, (content_id,))
            result = cursor.fetchall()
            paths = [row["file_path"] for row in result if row["file_path"]]
    except Exception as e:
        logger.error(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if connection:
            connection.close()
    return paths

# ==========================================================
# ğŸ©º í—¬ìŠ¤ì²´í¬
# ==========================================================
@app.get("/health/")
async def health_check():
    return {"status": "ok"}

# ==========================================================
# ğŸ“˜ ì „ì²´ ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸ (âœ… /contents ë¡œ prefix í†µì¼)
# ==========================================================
@app.post("/contents/{content_id}/summarize")
async def summarize_full(content_id: int):
    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")

    _, combined_content = combine_vectorstores(pdf_paths)
    if not combined_content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨")

    prompt = f"""
ë‹¤ìŒ PDF ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”.
í•µì‹¬ ê°œë… ìœ„ì£¼ë¡œ 5ë¬¸ë‹¨ ì´í•˜ë¡œ ì •ë¦¬:
---
{combined_content[:8000]}
"""
    result = llm.invoke(prompt)
    return {"content_id": content_id, "summaryText": result.content.strip()}

# ==========================================================
# ğŸ“— ë‹¨ì›ë³„ ìš”ì•½ (/contents/{id}/summaries)
# ==========================================================
@app.post("/contents/{content_id}/summaries")
async def summarize_chapter(content_id: int, request: Optional[ChapterRequest] = None):
    pdf_paths = request.pdf_paths if request and request.pdf_paths else get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")

    _, combined_content = combine_vectorstores(pdf_paths)
    if not combined_content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨")

    pattern = r"^(\d+\.\d+(\.\d+)?)\s*(.*)$"
    chapters = {}
    current_chapter = None
    for line in combined_content.splitlines():
        line = line.strip()
        if not line:
            continue
        match = re.match(pattern, line)
        if match:
            current_chapter = match.group(1)
            chapters[current_chapter] = match.group(3) + "\n"
        elif current_chapter:
            chapters[current_chapter] += line + "\n"

    summaries = []
    if request and request.chapter_request:
        matched = [k for k in chapters.keys() if k.startswith(request.chapter_request)]
        combined = "\n".join([chapters[k] for k in matched])
        result = llm.invoke(f"ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\n{combined[:8000]}")
        summaries.append({"chapter": request.chapter_request, "summaryText": result.content.strip()})
    else:
        for k, v in chapters.items():
            result = llm.invoke(f"{k} ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\n{v[:8000]}")
            summaries.append({"chapter": k, "summaryText": result.content.strip()})

    return {"summaries": summaries}

# ==========================================================
# â“ ì§ˆë¬¸ (RAG + ì›¹ë³´ì¡°)
# ==========================================================
@app.post("/contents/{content_id}/ask")
async def ask_question(content_id: int, body: QuestionRequest):
    question = body.question
    force_web = body.force_web

    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths and not tavily_tool:
        raise HTTPException(status_code=404, detail="PDF ë° ì›¹ ê²€ìƒ‰ ë¶ˆê°€")

    vectorstore, combined_text = combine_vectorstores(pdf_paths) if pdf_paths else (None, None)
    do_force_web = force_web or any(k in question for k in ["ì›¹", "ê²€ìƒ‰", "ì¸í„°ë„·"])

    if vectorstore and not do_force_web:
        retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
        rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
        response = rag_chain.invoke({"query": question})
        return {"answer": response.get("result", "ê²°ê³¼ ì—†ìŒ"), "source": "PDF"}

    if tavily_tool:
        resp = tavily_tool.invoke({"query": question})
        if isinstance(resp, dict) and "results" in resp:
            return {"answer": resp["results"][0].get("content", ""), "source": "WEB"}

    return {"answer": "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "source": "NONE"}

# ==========================================================
# ğŸ§  í€´ì¦ˆ ìƒì„± (/contents/{id}/quiz/generate)
# ==========================================================
@app.post("/contents/{content_id}/quiz/generate")
async def quiz_generate(content_id: int, req: QuizGenerationRequest):
    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")

    _, content = combine_vectorstores(pdf_paths)
    if not content:
        raise HTTPException(status_code=400, detail="PDF ë¡œë“œ ì‹¤íŒ¨")

    prompt = f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {req.num_questions}ê°œì˜ ê°ê´€ì‹ í€´ì¦ˆë¥¼ ìƒì„±í•˜ì„¸ìš”.
ë‚œì´ë„: {req.difficulty}
ì¶œë ¥ í˜•ì‹(JSON):
{{
"questions":[{{"question":"...", "options":["A","B","C","D"], "correct_answer":"...", "explanation":"..."}}]
}}
ë‚´ìš©:
{content[:10000]}
"""
    result = llm.invoke(prompt)
    cleaned = result.content.strip().replace("```json", "").replace("```", "")
    try:
        return json.loads(cleaned)
    except:
        return {"raw_response": cleaned}
