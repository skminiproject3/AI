import os
import re
import hashlib
import json
import logging
from typing import List, Tuple, Optional, Dict, Any
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_tavily import TavilySearch
import pymysql

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
# ==========================================================
# í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", "3306"))
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY ë¯¸ì„¤ì •!")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
tavily_tool = TavilySearch(api_key=TAVILY_API_KEY, max_results=3) if TAVILY_API_KEY else None

app = FastAPI(title="PDF í•™ìŠµ ë„ìš°ë¯¸ API")

origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,          # ê°œë°œ ì¤‘ì´ë©´ ["*"]ë„ ê°€ëŠ¥(credential ì•ˆ ì“¸ ë•Œ)
    allow_credentials=True,         # ì¿ í‚¤/ì¸ì¦ì •ë³´ ì“°ë©´ True
    allow_methods=["*"],            # ìµœì†Œí•œ ["POST","GET","OPTIONS"]ì—¬ë„ ë¨
    allow_headers=["*"],            # ë˜ëŠ” ["content-type","authorization"]
    expose_headers=["*"],           # (ì„ íƒ) í´ë¼ì—ì„œ ì½ì„ í—¤ë”
)

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

UPLOAD_DIR = "uploaded_pdfs"
VECTOR_DIR = "vector_stores"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(VECTOR_DIR, exist_ok=True)

# ==========================================================
# Pydantic ëª¨ë¸ ì •ì˜
# ==========================================================
class PdfPathsRequest(BaseModel):
    pdf_paths: List[str]

class ChapterRequest(BaseModel):
    pdf_paths: Optional[List[str]] = None
    chapter_request: Optional[str] = None
class QuestionRequest(BaseModel):
    question: str
    force_web: bool
    pdf_paths: Optional[List[str]] = None

class QuizGenerationRequest(BaseModel):
    pdf_paths: List[str]
    num_questions: int = Field(5, ge=1, le=20)
    difficulty: str = Field("MEDIUM", pattern="^(EASY|MEDIUM|HARD)$")
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        return "\n".join([p.page_content for p in pages])
    except Exception as e:
        logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path} | {e}")
        return ""

def get_or_create_vectorstore(pdf_path: str) -> Optional[FAISS]:
    normalized_path = pdf_path.replace("\\", "/")
    path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
    vector_path = os.path.join(VECTOR_DIR, path_hash)
    if os.path.exists(vector_path):
        return FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)
    try:
        loader = PyPDFLoader(pdf_path)
        texts = [p.page_content for p in loader.load()]
        vs = FAISS.from_texts(texts, embeddings)
        vs.save_local(vector_path)
        logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {pdf_path}")
        return vs
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ìƒì„± ì‹¤íŒ¨ ({pdf_path}): {e}")
        return None

def combine_vectorstores(pdf_paths: List[str]) -> Tuple[Optional[FAISS], Optional[str]]:
    vectorstores = []
    combined_content = ""
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            continue
        vs = get_or_create_vectorstore(pdf_path)
        if vs:
            vectorstores.append(vs)
            combined_content += extract_text_from_pdf(pdf_path) + "\n\n--- PDF ë¶„ë¦¬ ---\n\n"
    if not vectorstores:
        return None, None
    main_vs = vectorstores[0]
    for i in range(1, len(vectorstores)):
        main_vs.merge_from(vectorstores[i])
    return main_vs, combined_content.strip()

# ==========================================================
# ì±•í„° ê°ì§€ ë¡œì§
# ==========================================================
def detect_chapters_by_regex(text: str) -> List[str]:
    patterns = [
        r"ì œ\s*\d+\s*ì¥",
        r"CHAPTER\s+\d+",
        r"Chapter\s+\d+",
        r"\b\d+\.\d+",
        r"Section\s+\d+",
        r"Part\s+[IVXLC\d]+"
    ]
    found = set()
    for p in patterns:
        matches = re.findall(p, text)
        for m in matches:
            found.add(m.strip())
    return sorted(found)

def detect_chapters_by_llm(text: str) -> List[str]:
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì±•í„° ì œëª©ì„ ìˆœì„œëŒ€ë¡œ ë½‘ì•„ì£¼ì„¸ìš”.
ì˜ˆì‹œ ì¶œë ¥: ["ì œ1ì¥ ê°œìš”", "ì œ2ì¥ ì´ë¡ "]
---
{text[:4000]}
"""
    try:
        response = llm.invoke(prompt)
        lines = re.findall(r"ì œ?\s*\d+\s*ì¥|CHAPTER\s+\d+|Chapter\s+\d+|\d+\.\d+", response.content)
        return sorted(set(lines))
    except Exception as e:
        logger.warning(f"âš ï¸ LLM ì±•í„° ê°ì§€ ì‹¤íŒ¨: {e}")
        return []

def get_total_chapters(text: str) -> Dict[str, Any]:
    regex_chapters = detect_chapters_by_regex(text)
    if regex_chapters:
        return {"method": "regex", "total_chapters": len(regex_chapters), "chapter_list": regex_chapters}
    llm_chapters = detect_chapters_by_llm(text)
    if llm_chapters:
        return {"method": "llm", "total_chapters": len(llm_chapters), "chapter_list": llm_chapters}
    return {"method": "none", "total_chapters": 1, "chapter_list": []}

# ==========================================================
# DBì—ì„œ content_idë¡œ PDF ê²½ë¡œ ì¡°íšŒ
# ==========================================================
def get_pdf_paths_for_content(content_id: int):
    connection = None
    paths = []
    try:
        connection = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_DATABASE,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )
        with connection.cursor() as cursor:
            sql = "SELECT file_path FROM contents WHERE id=%s"
            cursor.execute(sql, (content_id,))
            result = cursor.fetchall()
            paths = [row['file_path'] for row in result if row['file_path']]
    except Exception as e:
        logger.error(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if connection:
            connection.close()
    return paths

# ==========================================================
# FastAPI ì´ˆê¸°í™”
# ==========================================================
app = FastAPI(title="PDF í•™ìŠµ ë„ìš°ë¯¸ API (Full í†µí•©)")

# ==========================================================
# âœ… /health
# ==========================================================
@app.get("/health/")
async def health_check():
    return {"status": "ok"}

# ==========================================================
# âœ… /upload_pdfs
# ==========================================================
@app.post("/upload_pdfs/")
async def upload_pdfs(files: List[UploadFile] = File(...)):
    if not files:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    logger.info("ğŸ“¥ ì—…ë¡œë“œ ìš”ì²­ ìˆ˜ì‹  | files=%d", len(files))

    saved_files: List[str] = []
    combined_text_parts: List[str] = []

    for file in files:
        save_path = os.path.join(UPLOAD_DIR, file.filename)
        try:
            # íŒŒì¼ ì €ì¥
            with open(save_path, "wb") as f:
                data = await file.read()
                f.write(data)
            saved_files.append(save_path)
            logger.info("ğŸ—‚ï¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ | path=%s | size=%d bytes", save_path, len(data))

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = extract_text_from_pdf(save_path)
            if text.strip():
                combined_text_parts.append(text)
            else:
                logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ë¬¸ì„œ | path=%s", save_path)

        except Exception as e:
            logger.error("âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ | file=%s | error=%s", file.filename, e)
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    if not combined_text_parts:
        raise HTTPException(status_code=400, detail="PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ì „ì²´ ë¬¸ì„œ ê¸°ì¤€ ì±•í„° ê°ì§€
    combined_text = "\n\n--- PDF ë¶„ë¦¬ ---\n\n".join(combined_text_parts)
    detection = get_total_chapters(combined_text)

    total_chapters_result: int = int(detection.get("total_chapters", 1))
    detected_method: str = str(detection.get("method", "none"))
    all_chapters: List[str] = list(detection.get("chapter_list", []))

    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° vector_path ê³„ì‚° (ë¬´ì¡°ê±´ ìƒì„±)
    vector_paths: List[str] = []
    for file_path in saved_files:
        vs = get_or_create_vectorstore(file_path)
        normalized_path = file_path.replace("\\", "/")
        path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
        vector_path = os.path.join(VECTOR_DIR, path_hash)
        vector_paths.append(vector_path)

    # ê¸°ë³¸ì ìœ¼ë¡œ í•˜ë‚˜ë§Œ ë°˜í™˜ (boot ì—°ë™ìš©)
    final_vector_path = vector_paths[0] if vector_paths else None
    logger.info(
        "âœ… ì—…ë¡œë“œ/ë¶„ì„ ì™„ë£Œ | files=%d | total_chapters=%d | method=%s | vector_path=%s",
        len(saved_files), total_chapters_result, detected_method, final_vector_path
    )

    return {
        "pdf_paths": saved_files,
        "total_chapters": total_chapters_result,
        "method": detected_method,
        "chapter_list": all_chapters,
        "vector_path": final_vector_path
    }
# ==========================================================
# âœ… /summarize (ì „ì²´ ìš”ì•½)
# ==========================================================
@app.post("/api/contents/{content_id}/summarize")
async def summarize_full(content_id: int):
    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")
    _, combined_content = combine_vectorstores(pdf_paths)
    if not combined_content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨")
    prompt = f"""
ë‹¤ìŒ PDF ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
í•µì‹¬ ìœ„ì£¼, 5ë¬¸ë‹¨ ì´í•˜.
---
{combined_content[:10000]}
"""
    result = llm.invoke(prompt)
    return {"content_id": content_id, "summaryText": result.content.strip()}

# ==========================================================
# âœ… /summaries (ë‹¨ì›ë³„ ìš”ì•½)
# ==========================================================
@app.post("/api/contents/{content_id}/summaries")
async def summarize_chapter(content_id: int, request: Optional[ChapterRequest] = None):
    """
    pdf_pathsë¥¼ ì•ˆ ë³´ë‚¸ ê²½ìš°, DBì—ì„œ ìë™ìœ¼ë¡œ í•´ë‹¹ content_idì˜ PDF ê²½ë¡œ ì¡°íšŒ
    """
    # pdf_paths ìœ íš¨ì„± í™•ì¸
    pdf_paths = []
    if request and request.pdf_paths:
        pdf_paths = request.pdf_paths
    else:
        pdf_paths = get_pdf_paths_for_content(content_id)  # âœ… DBì—ì„œ ê°€ì ¸ì˜¤ê¸°

    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # chapter_request ì½ê¸° (ì—†ìœ¼ë©´ ì „ì²´ ìš”ì•½)
    chapter_req = None
    if request and request.chapter_request:
        chapter_req = request.chapter_request

    # PDF ë‚´ìš© ë³‘í•©
    _, combined_content = combine_vectorstores(pdf_paths)
    if not combined_content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨")

    # ì±•í„°ë³„ ìš”ì•½ ìƒì„± (ì´ì „ ë¡œì§ ë™ì¼)
    pattern = r"^(\d+\.\d+(\.\d+)?)\s*(.*)$"
    chapters = {}
    current_chapter = None
    for line in combined_content.splitlines():
        line = line.strip()
        if not line:
            continue
        match = re.match(pattern, line)
        if match:
            current_chapter = match.group(1)
            chapters[current_chapter] = match.group(3) + "\n"
        elif current_chapter:
            chapters[current_chapter] += line + "\n"

    summaries = []
    if chapter_req:
        matched = [k for k in chapters.keys() if k.startswith(chapter_req)]
        combined = "\n".join([chapters[k] for k in matched])
        result = llm.invoke(f"ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\n{combined[:8000]}")
        summaries.append({"chapter": chapter_req, "summaryText": result.content.strip()})
    else:
        for k, v in chapters.items():
            result = llm.invoke(f"{k} ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\n{v[:8000]}")
            summaries.append({"chapter": k, "summaryText": result.content.strip()})

    return {"summaries": summaries}

# -----------------------
# LangChain Agent ê¸°ë°˜ ì§ˆë¬¸ ì—”ë“œí¬ì¸íŠ¸
# -----------------------
@app.post("/api/contents/{content_id}/ask")
async def ask_question(content_id: int, body: QuestionRequest):
    """
    - content_id: Boot DBì˜ contents.id
    - question: JSON bodyë¡œ ì „ì†¡ {"question": "...", "force_web": false}
    - force_web: (ì˜µì…˜) Trueë¡œ ë³´ë‚´ë©´ PDF í™•ì¸ ì—†ì´ ë°”ë¡œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    """
    question = body.question
    force_web = body.force_web

    logger.info(f"ì§ˆë¬¸ ìˆ˜ì‹ : content_id={content_id} / force_web={force_web} / question={question}")

    # DBì—ì„œ PDF ê²½ë¡œ ì¡°íšŒ
    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        logger.warning("PDF ê²½ë¡œ ì—†ìŒ: ì›¹ í´ë°± ì‹œë„")
        if not tavily_tool:
            raise HTTPException(status_code=404, detail="PDF ë° ì›¹ ê²€ìƒ‰ ë¶ˆê°€ (íŒŒì¼ ì—†ìŒ, Tavily ë¯¸ì„¤ì •)")
        pdf_paths = []

    # ectorstore ì¤€ë¹„
    vectorstore = None
    combined_text = None
    if pdf_paths:
        vectorstore, combined_text = combine_vectorstores(pdf_paths)
        if not vectorstore:
            logger.warning("Vectorstore ë¡œë“œ ì‹¤íŒ¨ â†’ ì›¹ í´ë°±")

    # force_web ë˜ëŠ” â€œì›¹ì—ì„œ/ê²€ìƒ‰â€ í‚¤ì›Œë“œ ìë™ ê°ì§€
    autotrig_web_keywords = any(k in question for k in ["ì›¹ì—ì„œ", "ì¸í„°ë„·", "ê²€ìƒ‰", "ì›¹ìœ¼ë¡œ"])
    do_force_web = force_web or autotrig_web_keywords

    # PDF RAG ì‹œë„ (force_web=False ì¼ ë•Œë§Œ)
    if vectorstore and not do_force_web:
        try:
            retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
            rag_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=retriever,
                return_source_documents=True
            )

            pdf_response = rag_chain.invoke({"query": question})
            logger.debug(f"PDF RAG ì‘ë‹µ: keys={list(pdf_response.keys()) if isinstance(pdf_response, dict) else 'N/A'}")

            # PDF ê²°ê³¼ ì •ë¦¬
            result_text = ""
            sources = []
            if isinstance(pdf_response, dict):
                result_text = str(pdf_response.get("result") or pdf_response.get("output_text") or "")
                sources = pdf_response.get("source_documents") or []
            else:
                result_text = str(pdf_response)

            # ê²°ê³¼ ê²€ì¦
            if sources and len(result_text.strip()) > 10:
                refs = []
                for s in sources[:3]:
                    try:
                        meta = getattr(s, "metadata", {}) or (s.get("metadata") if isinstance(s, dict) else {})
                        snippet = (getattr(s, "page_content", "") or s.get("page_content", ""))[:300]
                        refs.append({"metadata": meta, "snippet": snippet})
                    except Exception:
                        refs.append({"raw": str(s)[:300]})
                return {
                    "source": "PDF",
                    "content_id": content_id,
                    "question": question,
                    "answer": result_text.strip(),
                    "pdf_reference_count": len(sources),
                    "pdf_references": refs,
                    "message": "PDF ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                }
            else:
                logger.info("PDFì—ì„œ ìœ ì˜ë¯¸í•œ ê·¼ê±° ëª»ì°¾ìŒ â†’ ì›¹ í´ë°±")
        except Exception as e:
            logger.exception(f"PDF RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ì‹œ ì›¹ê²€ìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°

    # ì›¹ í´ë°± (Tavily ê²€ìƒ‰)
    if tavily_tool:
        try:
            query_clean = re.sub(r"(ì›¹ì—ì„œ|ì›¹ìœ¼ë¡œ|ì¸í„°ë„·|ê²€ìƒ‰|í•´ì¤˜|ì•Œë ¤ì¤˜)", "", question, flags=re.IGNORECASE).strip()
            if not query_clean:
                query_clean = question

            web_resp = tavily_tool.invoke({"query": query_clean})
            results = web_resp.get("results", []) if isinstance(web_resp, dict) else web_resp

            if results:
                sorted_results = sorted(results, key=lambda x: x.get("score", 0), reverse=True)
                best = sorted_results[0]
                title = best.get("title") or best.get("url") or "ì¶œì²˜ ì—†ìŒ"
                url = best.get("url", "")
                extracted = best.get("content") or best.get("snippet") or ""

                if extracted and len(extracted) > 30:
                    prompt = f"""
ì•„ë˜ ì¶œì²˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•˜ê³  ì •í™•í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
ì¶œì²˜ ì œëª©: {title}
ì¶œì²˜ URL: {url}
ì¶œì²˜ ë‚´ìš©:
{extracted}

ì§ˆë¬¸: {question}

- ë‹µë³€ì€ 3~6 ë¬¸ì¥ ì´ë‚´ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ì¶œì²˜ URLì„ 'reference' í•„ë“œì— í¬í•¨ì‹œí‚¤ì„¸ìš”.
"""
                    summary = llm.invoke(prompt).content.strip()
                else:
                    summary = extracted or str(best)[:400]

                return {
                    "source": "WEB",
                    "content_id": content_id,
                    "question": question,
                    "answer": summary,
                    "reference": {"title": title, "url": url},
                    "message": "ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤."
                }

            logger.info("ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        except Exception as e:
            logger.exception(f"ì›¹ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨ ì‹œ
    return {
        "source": "NONE",
        "content_id": content_id,
        "question": question,
        "answer": "ê´€ë ¨ ì •ë³´ë¥¼ PDF ë° ì›¹ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "message": "ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ì™¸ë¶€ ì˜ì¡´ì„± ë¯¸ì„¤ì •"
    }

# -----------------------
# í€´ì¦ˆ ìƒì„±
# -----------------------
@app.post("/api/contents/{content_id}/quiz/generate")
async def quiz_generate(content_id: int, request: dict):
    try:
        # Bootì—ì„œ ì „ë‹¬í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        num_questions = request.get("num_questions")
        difficulty = request.get("difficulty")

        if not num_questions or not difficulty:
            raise HTTPException(status_code=400, detail="num_questions, difficulty í•„ìˆ˜")

        # âœ… Boot-DBì—ì„œ PDF ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        saved_pdfs = get_pdf_paths_for_content(content_id)
        if not saved_pdfs:
            raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")

        # âœ… PDFì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
        vectorstore, content = combine_vectorstores(saved_pdfs)
        if not content:
            raise HTTPException(status_code=400, detail="PDF ë¡œë“œ ì‹¤íŒ¨")

        # âœ… LLM ìš”ì²­ Prompt
        prompt = f"""
ë‹¹ì‹ ì€ ì‹œí—˜ ë¬¸ì œë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ í€´ì¦ˆ {num_questions}ê°œ ìƒì„±í•˜ì„¸ìš”.
ë‚œì´ë„: {difficulty}

ì¶œë ¥ í˜•ì‹(JSON):
{{
"questions":[
    {{
    "question": "ë¬¸ì œ",
    "options": ["A", "B", "C", "D"],
    "correct_answer": "ì •ë‹µ",
    "explanation": "ì •ë‹µ í•´ì„¤"
    }}
]}}
ë‚´ìš©:
{content[:12000]}
"""

        result = llm.invoke(prompt)

        try:
            cleaned = result.content.strip().replace("```json","").replace("```","")
            data = json.loads(cleaned)
            return data  # âœ… Bootê°€ ì›í•˜ëŠ” êµ¬ì¡° ê·¸ëŒ€ë¡œ ë°˜í™˜

        except Exception as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ì›ë³¸ ë°˜í™˜: {e}")
            return {"raw_response": result.content.strip()}

    except Exception as e:
        logger.error(f"í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))