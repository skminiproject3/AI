import os
import re
import hashlib
import json
import logging
from typing import List, Tuple, Optional, Dict
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from importlib_metadata import metadata
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.agents import initialize_agent, Tool
from langchain.chains import RetrievalQA
from langchain_tavily import TavilySearch
from fastapi import Form, Body
import pymysql
# -----------------------
# í™˜ê²½ ì„¤ì •
# -----------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT"))
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USERNAME")
DB_PASSWORD = os.getenv("DB_PASSWORD")
if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY ë¯¸ì„¤ì •!")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
tavily_tool = TavilySearch(api_key=TAVILY_API_KEY, max_results=3) if TAVILY_API_KEY else None

# -----------------------
# ë¡œê¹… ì„¤ì •
# -----------------------
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# -----------------------
# ì—…ë¡œë“œ ë° ë²¡í„° DB í´ë”
# -----------------------
UPLOAD_DIR = "uploaded_pdfs"
VECTOR_DIR = "vector_stores"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(VECTOR_DIR, exist_ok=True)

# -----------------------
# Pydantic ëª¨ë¸
# -----------------------
class PdfPathsRequest(BaseModel):
    pdf_paths: List[str]
class AskRequest(BaseModel):
    question: str
class ChapterRequest(BaseModel):
    pdf_paths: Optional[List[str]] = None
    chapter_request: Optional[str] = None

class QuestionRequest(BaseModel):
    question: str
    pdf_paths: List[str]

class QuizGenerationRequest(BaseModel):
    pdf_paths: List[str]
    num_questions: int = Field(5, ge=1, le=20)
    difficulty: str = Field("MEDIUM", pattern="^(EASY|MEDIUM|HARD)$")
# -----------------------
# PDF ì²˜ë¦¬ ë° ë²¡í„°ìŠ¤í† ì–´
# -----------------------
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        return "\n".join([p.page_content for p in pages])
    except Exception as e:
        logger.error(f"PDF ì¶”ì¶œ ì‹¤íŒ¨ ({pdf_path}): {e}")
        return ""
def recognize_chapters_with_llm(full_text: str) -> List[Dict]:
    first_match = re.search(r"^(\d+)\s", full_text, re.MULTILINE)
    chapters = []
    if first_match:
        chapters.append({"chapter": first_match.group(1), "title": ""})

    try:
        prompt = f"""
ì•„ë˜ëŠ” PDF í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ì—ì„œ 'ì¥ ë²ˆí˜¸'ì™€ 'ì¥ ì œëª©'ì„ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.
ì˜ˆ: {{ "chapters": [{{"chapter": "1", "title": "ì•”í˜¸ ê°œë¡ "}}] }}

í…ìŠ¤íŠ¸:
{full_text[:5000]}
"""
        resp = llm.invoke(prompt)
        json_text = re.sub(r"```json|```", "", resp.content.strip())
        llm_chapters = json.loads(json_text).get("chapters", [])
        chapters = llm_chapters if llm_chapters else chapters
    except Exception as e:
        logger.warning(f"LLM ì±•í„° ì¸ì‹ ì‹¤íŒ¨: {e}")
    
    return chapters

def get_ordered_subchapters(metadata: dict) -> List[str]:
    """PDF ë“±ì¥ ìˆœì„œ ê·¸ëŒ€ë¡œ ë‹¨ì› ë²ˆí˜¸ ë°˜í™˜"""
    chapters = metadata.get("chapters", [])
    return [ch["chapter"] for ch in chapters if "chapter" in ch]

def get_or_create_vectorstore(pdf_path: str) -> Optional[FAISS]:
    # normalized_path = pdf_path.replace("\\","/")
    # path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
    # vector_path = os.path.join(VECTOR_DIR, path_hash)
    # if os.path.exists(vector_path):
    #     return FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)
    # try:
    #     loader = PyPDFLoader(pdf_path)
    #     texts = [p.page_content for p in loader.load()]
    #     if not texts:
    #         return None
    #     vs = FAISS.from_texts(texts, embeddings)
    #     vs.save_local(vector_path)
    #     logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {pdf_path}")
    #     return vs
    # except Exception as e:
    #     logger.error(f"âŒ ë²¡í„° ìƒì„± ì‹¤íŒ¨ ({pdf_path}): {e}")
    #     return None
    normalized_path = pdf_path.replace("\\", "/")
    path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
    vector_path = os.path.join(VECTOR_DIR, path_hash)
    metadata_path = os.path.join(vector_path, "metadata.json")

    if os.path.exists(vector_path):
        logger.info(f"ğŸ“‚ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ: {pdf_path}")
        return FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)

    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        texts = [p.page_content for p in pages]
        if not texts:
            raise ValueError("PDFì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        full_text = "\n".join(texts)
        chapters = recognize_chapters_with_llm(full_text)

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vs = FAISS.from_texts(texts, embeddings)
        os.makedirs(vector_path, exist_ok=True)
        vs.save_local(vector_path)

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump({"chapters": chapters}, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ + ì±•í„° ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {pdf_path}")
        return vs

    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ìƒì„± ì‹¤íŒ¨ ({pdf_path}): {e}")
        return None


def combine_vectorstores(pdf_paths: List[str]) -> Tuple[Optional[FAISS], Optional[str]]:
    vectorstores = []
    combined_content = ""
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            continue
        vs = get_or_create_vectorstore(pdf_path)
        if vs:
            vectorstores.append(vs)
            combined_content += extract_text_from_pdf(pdf_path) + "\n\n--- PDF ë¶„ë¦¬ ---\n\n"
    if not vectorstores:
        return None, None
    main_vs = vectorstores[0]
    for i in range(1, len(vectorstores)):
        main_vs.merge_from(vectorstores[i])
    return main_vs, combined_content.strip()

def split_by_subchapter(text: str) -> Dict[str,str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì›ë³„ë¡œ ë¶„ë¦¬ (ìˆ«ì.ìˆ«ì ë˜ëŠ” ìˆ«ì.ìˆ«ì.ìˆ«ì)"""
    pattern = r"^(\d+\.\d+(\.\d+)?)\s*(.*)$"  # ìˆ«ì.ìˆ«ì + optional ë‹¨ì› ì œëª©
    chapters = {}
    current_chapter = None
    
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        match = re.match(pattern, line)
        if match:
            current_chapter = match.group(1)  # "4.3"ë§Œ í‚¤ë¡œ ì‚¬ìš©
            chapters[current_chapter] = match.group(3) + "\n"  # ì œëª© í¬í•¨
        elif current_chapter:
            chapters[current_chapter] += line + "\n"
    return chapters
# PDF ê²½ë¡œë¥¼ bootì—ì„œ ì „ë‹¬ë°›ëŠ”  í•¨ìˆ˜ (Bootì—ì„œ DB ì¡°íšŒ í›„ ì „ë‹¬)
# -----------------------
# PDF ê²½ë¡œ ì¡°íšŒ í•¨ìˆ˜
# -----------------------
def get_pdf_paths_for_content(content_id: int):
    """
    content_id ê¸°ì¤€ìœ¼ë¡œ MariaDBì—ì„œ file_pathë¥¼ ì¡°íšŒ
    ë°˜í™˜: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    connection = None
    paths = []

    try:
        connection = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_DATABASE,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        with connection.cursor() as cursor:
            sql = "SELECT file_path FROM contents WHERE id=%s"
            cursor.execute(sql, (content_id,))
            result = cursor.fetchall()
            paths = [row['file_path'] for row in result if row['file_path']]
    except Exception as e:
        print(f"âŒ PDF ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if connection:
            connection.close()

    return 
def get_vector_paths_for_content(content_id: int) -> List[str]:
    """
    content_id ê¸°ì¤€ìœ¼ë¡œ MariaDBì—ì„œ vector_pathë¥¼ ì¡°íšŒ
    ë°˜í™˜: ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    connection = None
    paths = []

    try:
        connection = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_DATABASE,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        with connection.cursor() as cursor:
            sql = "SELECT vector_path FROM contents WHERE id=%s"
            cursor.execute(sql, (content_id,))
            result = cursor.fetchall()
            paths = [row['vector_path'] for row in result if row['vector_path']]
    except Exception as e:
        print(f"âŒ ë²¡í„° ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if connection:
            connection.close()

    return paths

def query_web(question: str):
    """
    ì›¹ ê²€ìƒ‰ í›„ LLMìœ¼ë¡œ í•™ìŠµ ê´€ë ¨ ì •ë³´ ìš”ì•½
    """
    try:
        # ì§ˆë¬¸ì—ì„œ "ì›¹", "ê²€ìƒ‰" ë“± í‚¤ì›Œë“œ ì œê±°
        query_clean = re.sub(r"(ì›¹ì—ì„œ|ì›¹ìœ¼ë¡œ|ì¸í„°ë„·|ê²€ìƒ‰|ì•Œë ¤ì¤˜|í•´ì¤˜)", "", question, flags=re.IGNORECASE).strip()
        if not query_clean:
            query_clean = question

        web_resp = tavily_tool.invoke({"query": query_clean})
        results = web_resp.get("results", []) if isinstance(web_resp, dict) else (web_resp if isinstance(web_resp, list) else [])

        if not results:
            return None

        # ìµœê³  ì ìˆ˜ ê²°ê³¼ ì„ íƒ
        best = sorted(results, key=lambda x: x.get("score", 0), reverse=True)[0]
        title = best.get("title") or best.get("url") or "ì¶œì²˜ ì—†ìŒ"
        url = best.get("url") or ""
        snippet = best.get("content") or best.get("snippet") or str(best)[:400]

        # LLMìœ¼ë¡œ í•™ìŠµ ê´€ë ¨ ì •ë³´ë§Œ ìš”ì•½
        summary_prompt = f"""
ì•„ë˜ ì¶œì²˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
- ë‹µë³€ì€ í•™ìŠµ ê´€ë ¨ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±
ì¶œì²˜ ì œëª©: {title}
ì¶œì²˜ URL: {url}
ë‚´ìš©: {snippet}
ì§ˆë¬¸: {question}
"""
        answer = llm.invoke(summary_prompt).content.strip()
        if url:
            answer += f"\n\nì¶œì²˜: {url}"

        return answer
    except Exception as e:
        logger.exception(f"ì›¹ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def query_pdf_rag(vectorstore, question: str) -> str:
    """
    PDF RAG ê¸°ë°˜ ë‹µë³€
    """
    try:
        retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
        rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
        pdf_response = rag_chain.invoke({"query": question})

        # PDF ê²°ê³¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        if isinstance(pdf_response, dict):
            result_text = str(pdf_response.get("result") or pdf_response.get("output_text") or "")
            sources = pdf_response.get("source_documents") or []
        else:
            result_text = str(pdf_response)
            sources = []

        if sources and len(result_text.strip()) > 10:
            return result_text.strip()
        else:
            return None
    except Exception as e:
        logger.exception(f"PDF RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
async def ask_question_logic(content_id: int, question: str, force_web: bool) -> Dict:
    """
    content_id: Boot DB contents.id
    question: ì§ˆë¬¸ í…ìŠ¤íŠ¸
    force_web: Trueì´ë©´ PDF ë¬´ì‹œí•˜ê³  ì›¹ ê²€ìƒ‰ ê°•ì œ
    ë°˜í™˜: dict {"source":"PDF"/"WEB"/"NONE", "answer":..., "reference":..., "message":...}
    """

    # 1) PDF ê²½ë¡œ ì¡°íšŒ
    pdf_paths = get_pdf_paths_for_content(content_id)
    vectorstore, combined_text = (None, None)
    if pdf_paths:
        vectorstore, combined_text = combine_vectorstores(pdf_paths)

    # 2) PDF ê¸°ë°˜ RAG
    if vectorstore and not force_web:
        try:
            retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
            rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
            pdf_resp = rag_chain.invoke({"query": question})

            # PDF ì‘ë‹µ íŒŒì‹±
            answer_text = ""
            sources = []
            if isinstance(pdf_resp, dict):
                answer_text = str(pdf_resp.get("result") or pdf_resp.get("output_text") or "")
                sources = pdf_resp.get("source_documents") or []
            else:
                answer_text = str(pdf_resp)

            if sources and len(answer_text.strip()) > 10:
                refs = []
                for s in sources[:3]:
                    meta = getattr(s, "metadata", {}) or (s.get("metadata") if isinstance(s, dict) else {})
                    snippet = getattr(s, "page_content", "") or s.get("page_content", "")
                    refs.append({"metadata": meta, "snippet": snippet[:300]})
                return {"source": "PDF", "answer": answer_text.strip(), "pdf_references": refs, "message": "PDF ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."}
        except Exception as e:
            logger.exception(f"PDF RAG ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    # 3) ì›¹ í´ë°± (Tavily)
    if tavily_tool:
        try:
            query_clean = re.sub(r"(ì›¹ì—ì„œ|ì›¹ìœ¼ë¡œ|ì¸í„°ë„·|ê²€ìƒ‰|í•´ì¤˜|ì•Œë ¤ì¤˜)", "", question, flags=re.IGNORECASE).strip()
            query_clean = query_clean or question
            web_resp = tavily_tool.invoke({"query": query_clean})
            results = web_resp.get("results", []) if isinstance(web_resp, dict) else web_resp

            if results:
                best = sorted(results, key=lambda x: x.get("score",0), reverse=True)[0]
                title = best.get("title") or best.get("url") or "ì¶œì²˜ ì—†ìŒ"
                url = best.get("url", "")
                content = best.get("content") or best.get("snippet") or str(best)[:400]

                if len(content) > 30:
                    prompt = f"""
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ ì‘ì„± (í•œêµ­ì–´)
ì¶œì²˜ ì œëª©: {title}
ì¶œì²˜ URL: {url}
ì¶œì²˜ ë‚´ìš©: {content}
ì§ˆë¬¸: {question}
- ë‹µë³€ 3~6ë¬¸ì¥
- referenceì— URL í¬í•¨
"""
                    summary = llm.invoke(prompt).content.strip()
                else:
                    summary = content

                return {"source": "WEB", "answer": summary, "reference": {"title": title, "url": url}, "message": "ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€"}
        except Exception as e:
            logger.exception(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    # 4) ì‹¤íŒ¨
    return {"source": "NONE", "answer": "PDF ë° ì›¹ì—ì„œ ê´€ë ¨ ì •ë³´ ì—†ìŒ", "message": "ê²€ìƒ‰ ì‹¤íŒ¨"}
# -----------------------
# ìš”ì•½ í”„ë¡¬í”„íŠ¸
# -----------------------
summary_prompt = PromptTemplate(
    input_variables=["content"],
    template="ë‹¹ì‹ ì€ êµì¬ í•µì‹¬ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‚´ìš©: {content}"
)

bulk_summary_prompt = PromptTemplate(
    input_variables=["content", "chapters_list"],
    template="êµì¬ ë‚´ìš©ì„ ë‹¨ì›ë³„ ìš”ì•½. chapters_list={chapters_list} ë‚´ìš©={content}"
)

# -----------------------
# FastAPI ì´ˆê¸°í™”
# -----------------------
app = FastAPI(title="PDF í•™ìŠµ ë„ìš°ë¯¸ API (Agent + RAG)")

# -----------------------
# ì—”ë“œí¬ì¸íŠ¸: Health
# -----------------------
@app.get("/health/")
async def health_check():
    return {"status": "ok"}

# -----------------------
# ì—”ë“œí¬ì¸íŠ¸: PDF ì—…ë¡œë“œ
# -----------------------
@app.post("/api/contents/upload")
async def upload_pdfs(files: List[UploadFile] = File(...)):
    response_list = []

    for file in files:
        save_path = os.path.join(UPLOAD_DIR, file.filename)
        try:
            # 1ï¸âƒ£ íŒŒì¼ ì €ì¥
            with open(save_path, "wb") as f:
                f.write(await file.read())

            # 2ï¸âƒ£ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vs = get_or_create_vectorstore(save_path)
            vector_path = None
            if vs:
                # vector_path ê³„ì‚°
                normalized_path = save_path.replace("\\", "/")
                path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
                vector_path = os.path.join(VECTOR_DIR, path_hash)

            # 3ï¸âƒ£ Bootì—ì„œ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ê°ì²´ ìƒì„±
            response_list.append({
                "contentId": None,  # Bootì—ì„œ DB ì €ì¥ í›„ ì±„ì›Œì•¼ í•˜ëŠ” ê²½ìš°
                "title": file.filename,
                "status": "COMPLETED",
                "vectorId": None,
                "vector_path": vector_path
            })

        except Exception as e:
            return JSONResponse(status_code=500, content={"error": str(e)})

    return response_list

# -----------------------
# ì—”ë“œí¬ì¸íŠ¸: ì „ì²´ ìš”ì•½
# -----------------------
@app.post("/api/contents/{content_id}/summarize")
async def summarize_full(content_id: int):
    """
    content_id: Boot DB contents.id
    Boot DBì—ì„œ vector_pathë¥¼ ê°€ì ¸ì™€ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë‚´ìš© ë¡œë“œ í›„ ì „ì²´ ìš”ì•½
    """
    try:
        # 1ï¸âƒ£ DBì—ì„œ vector_path ì¡°íšŒ
        vector_paths = get_vector_paths_for_content(content_id)
        if not vector_paths:
            raise HTTPException(status_code=404, detail="ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")

        # 2ï¸âƒ£ vector_path ê¸°ë°˜ FAISS ë¡œë“œ
        vectorstores = []
        for vp in vector_paths:
            if os.path.exists(vp):
                try:
                    vs = FAISS.load_local(vp, embeddings, allow_dangerous_deserialization=True)
                    vectorstores.append(vs)
                except Exception as e:
                    logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨ ({vp}): {e}")

        if not vectorstores:
            raise HTTPException(status_code=500, detail="ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")

        # 3ï¸âƒ£ ì—¬ëŸ¬ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ í•©ì¹˜ê¸°
        main_vs = vectorstores[0]
        for vs in vectorstores[1:]:
            main_vs.merge_from(vs)

        # 4ï¸âƒ£ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ í™•ë³´
        retriever = main_vs.as_retriever(search_kwargs={"k": 5})
        rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
        pdf_resp = rag_chain.invoke({"query": "ì „ì²´ ë‚´ìš© ìš”ì•½"})  # ì „ì²´ ë‚´ìš© í™•ë³´ìš©
        if isinstance(pdf_resp, dict):
            combined_content = str(pdf_resp.get("result") or pdf_resp.get("output_text") or "")
        else:
            combined_content = str(pdf_resp)

        if not combined_content or len(combined_content.strip()) < 20:
            raise HTTPException(status_code=500, detail="ë²¡í„°ìŠ¤í† ì–´ ë‚´ìš© ë¶€ì¡±")

        # 5ï¸âƒ£ LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
        prompt = f"""
ë‹¤ìŒ PDF ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
- ìµœëŒ€ 5ë¬¸ë‹¨ ë‚´ì™¸
- ê°„ê²°í•˜ê³  í•µì‹¬ ìœ„ì£¼
- ë‚´ìš©ì— ê¸°ë°˜í•œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ í¬í•¨

ë‚´ìš©:
{combined_content[:10000]}
"""
        result = llm.invoke(prompt)

        return {
            "content_id": content_id,
            "summaryText": result.content.strip()
        }

    except Exception as e:
        logger.error(f"ì „ì²´ ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# -----------------------
# ì—”ë“œí¬ì¸íŠ¸: ë‹¨ì›ë³„ ìš”ì•½
# -----------------------
@app.post("/api/contents/{content_id}/summaries")
async def summarize_chapter(content_id: int, request: ChapterRequest):
    vector_paths = get_vector_paths_for_content(content_id)
    if not vector_paths:
        raise HTTPException(status_code=404, detail="ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì—†ìŒ")

    combined_text = ""
    subchapters_ordered = []

    for vp in vector_paths:
        metadata_path = os.path.join(vp, "metadata.json")
        vs = FAISS.load_local(vp, embeddings, allow_dangerous_deserialization=True)
        for doc in vs.docstore._dict.values():
            combined_text += getattr(doc, "page_content", "") + "\n"
        if os.path.exists(metadata_path):
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
                subchapters_ordered.extend(get_ordered_subchapters(metadata))

    if not request.chapter_request:
        raise HTTPException(status_code=400, detail="chapter_request í•„ìš”")

    requested_idx = int(request.chapter_request) - 1
    if requested_idx < 0 or requested_idx >= len(subchapters_ordered):
        raise HTTPException(status_code=404, detail="ìš”ì²­ ë‹¨ì› ì—†ìŒ")

    target_subchapter = subchapters_ordered[requested_idx]

    # ë‹¨ì› í…ìŠ¤íŠ¸ ì¶”ì¶œ
    start_idx = combined_text.find(target_subchapter)
    if requested_idx + 1 < len(subchapters_ordered):
        next_chapter = subchapters_ordered[requested_idx + 1]
        end_idx = combined_text.find(next_chapter, start_idx)
        if end_idx == -1:
            end_idx = len(combined_text)
    else:
        end_idx = len(combined_text)
    chapter_text = combined_text[start_idx:end_idx]

    # LLM ìš”ì•½
    prompt = f"""
ë‹¤ìŒì€ {target_subchapter}ì— í•´ë‹¹í•˜ëŠ” êµì¬ ë‚´ìš©ì…ë‹ˆë‹¤.
í•µì‹¬ ë‚´ìš©ì„ 3ë¬¸ë‹¨ ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ë‚´ìš©:
{chapter_text[:12000]}
"""
    result = llm.invoke(prompt)

    return {
        "summaries": [
            {
                "chapter": request.chapter_request,
                "summaryText": result.content.strip()
            }
        ]
    }
# -----------------------
# LangChain Agent ê¸°ë°˜ ì§ˆë¬¸ ì—”ë“œí¬ì¸íŠ¸
# -----------------------
@app.post("/api/contents/{content_id}/ask")
async def ask_question(content_id: int, req: AskRequest):
    question = req.question
    # 1ï¸âƒ£ DBì—ì„œ vector_path ì¡°íšŒ
    vector_paths = get_vector_paths_for_content(content_id)
    vectorstore = None
    answer = None
    if vector_paths:
        # ì—¬ëŸ¬ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ í•©ì¹˜ê¸°
        vectorstores = []
        for vp in vector_paths:
            if os.path.exists(vp):
                try:
                    vs = FAISS.load_local(vp, embeddings, allow_dangerous_deserialization=True)
                    vectorstores.append(vs)
                except Exception as e:
                    logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨ ({vp}): {e}")
        if vectorstores:
            main_vs = vectorstores[0]
            for vs in vectorstores[1:]:
                main_vs.merge_from(vs)
            vectorstore = main_vs


    # ì›¹ í´ë°± ì—¬ë¶€ íŒë‹¨
    do_force_web = any(k in question for k in ["ì›¹ì—ì„œ", "ê²€ìƒ‰", "ì¸í„°ë„·", "ì•Œë ¤ì¤˜", "ì›¹ìœ¼ë¡œ"])
    if not answer or do_force_web:
        web_answer = query_web(question)
        if web_answer:
            answer = web_answer

    # ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
    if not answer:
        answer = "ê´€ë ¨ ì •ë³´ë¥¼ PDF ë° ì›¹ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return answer

# -----------------------
# í€´ì¦ˆ ìƒì„±
# -----------------------
@app.post("/api/contents/{content_id}/quiz/generate")
async def quiz_generate(content_id: int, request: dict):
    try:
        # Bootì—ì„œ ì „ë‹¬í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        num_questions = request.get("num_questions")
        difficulty = request.get("difficulty")

        if not num_questions or not difficulty:
            raise HTTPException(status_code=400, detail="num_questions, difficulty í•„ìˆ˜")

        # 1ï¸âƒ£ DBì—ì„œ vector_path ì¡°íšŒ
        vector_paths = get_vector_paths_for_content(content_id)
        if not vector_paths:
            raise HTTPException(status_code=404, detail="ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì—†ìŒ")

        # 2ï¸âƒ£ vector_path ê¸°ë°˜ FAISS ë¡œë“œ
        vectorstores = []
        for vp in vector_paths:
            if os.path.exists(vp):
                try:
                    vs = FAISS.load_local(vp, embeddings, allow_dangerous_deserialization=True)
                    vectorstores.append(vs)
                except Exception as e:
                    logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨ ({vp}): {e}")

        if not vectorstores:
            raise HTTPException(status_code=500, detail="ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")
        # 3ï¸âƒ£ ì—¬ëŸ¬ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ í•©ì¹˜ê¸°
        main_vs = vectorstores[0]
        for vs in vectorstores[1:]:
            main_vs.merge_from(vs)
            
        # 4ï¸âƒ£ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê²€ìƒ‰ìš©)
        retriever = main_vs.as_retriever(search_kwargs={"k": 5})
        rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
        pdf_resp = rag_chain.invoke({"query": "ì „ì²´ ë‚´ìš© ìš”ì•½"})  # ì „ì²´ ë‚´ìš© í™•ë³´ìš©
        if isinstance(pdf_resp, dict):
            content = str(pdf_resp.get("result") or pdf_resp.get("output_text") or "")
        else:
            content = str(pdf_resp)

        if not content or len(content.strip()) < 20:
            raise HTTPException(status_code=500, detail="ë²¡í„°ìŠ¤í† ì–´ ë‚´ìš© ë¶€ì¡±")
        
        # âœ… LLM ìš”ì²­ Prompt
        prompt = f"""
ë‹¹ì‹ ì€ ì‹œí—˜ ë¬¸ì œë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ í€´ì¦ˆ {num_questions}ê°œ ìƒì„±í•˜ì„¸ìš”.
ë‚œì´ë„: {difficulty}

ì¶œë ¥ í˜•ì‹(JSON):
{{
"questions":[
    {{
    "question": "ë¬¸ì œ",
    "options": ["A", "B", "C", "D"],
    "correct_answer": "ì •ë‹µ",
    "explanation": "ì •ë‹µ í•´ì„¤"
    }}
]}}
ë‚´ìš©:
{content[:12000]}
"""

        result = llm.invoke(prompt)

        try:
            cleaned = result.content.strip().replace("```json","").replace("```","")
            data = json.loads(cleaned)
            return data  # âœ… Bootê°€ ì›í•˜ëŠ” êµ¬ì¡° ê·¸ëŒ€ë¡œ ë°˜í™˜

        except Exception as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ì›ë³¸ ë°˜í™˜: {e}")
            return {"raw_response": result.content.strip()}

    except Exception as e:
        logger.error(f"í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

