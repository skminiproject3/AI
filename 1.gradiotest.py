# main.py
import os
import json
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ í™•ì¸ í•„ìš”")

# LangChain íŒ¨í‚¤ì§€
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# Gradio
import gradio as gr
from gradio_pdf import PDF

# ì „ì—­ ë³€ìˆ˜
current_vectorstore = None
current_pdf_path = None
current_questions = []
current_answers = []

# ----------------------------
# PDF ë¡œë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
# ----------------------------
def load_pdf_to_vector_store(pdf_file, chunk_size=1000, chunk_overlap=100):
    global current_vectorstore, current_pdf_path
    if current_vectorstore is not None and current_pdf_path == pdf_file:
        return current_vectorstore

    loader = PyPDFLoader(pdf_file)
    documents = loader.load()
    if not documents:
        raise ValueError("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=int(chunk_size),
        chunk_overlap=int(chunk_overlap),
        separators=["\n\n", "\n", ".", " ", ""]
    )
    splits = text_splitter.split_documents(documents)

    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model="text-embedding-3-small")
    vectorstore = FAISS.from_documents(splits, embeddings)

    current_vectorstore = vectorstore
    current_pdf_path = pdf_file
    return vectorstore

# ----------------------------
# PDF ìš”ì•½
# ----------------------------
def summarize_pdf(pdf_file):
    try:
        vectorstore = load_pdf_to_vector_store(pdf_file)
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

        template = "ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n{context}\nìš”ì•½:"
        prompt = ChatPromptTemplate.from_template(template)
        model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, api_key=OPENAI_API_KEY)
        doc_chain = create_stuff_documents_chain(model, prompt)
        chain = create_retrieval_chain(retriever, doc_chain)
        result = chain.invoke({"input": "ìš”ì•½"})
        return result['answer']
    except Exception as e:
        return f"PDF ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ----------------------------
# ì—°ìŠµë¬¸ì œ ìƒì„± (ë¬¸ì œ/ì •ë‹µ ë¶„ë¦¬)
# ----------------------------
def generate_exercises(pdf_file):
    global current_questions, current_answers
    try:
        vectorstore = load_pdf_to_vector_store(pdf_file)
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

        template = '''
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ 3ê°œì˜ ì—°ìŠµë¬¸ì œë¥¼ ë§Œë“¤ê³  ê° ë¬¸ì œì˜ ì •ë‹µì„ JSON í˜•íƒœë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ:
[
    {"question": "ë¬¸ì œ 1 ë‚´ìš©", "answer": "ì •ë‹µ 1"},
    {"question": "ë¬¸ì œ 2 ë‚´ìš©", "answer": "ì •ë‹µ 2"},
    {"question": "ë¬¸ì œ 3 ë‚´ìš©", "answer": "ì •ë‹µ 3"}
]

ë¬¸ì„œ:
{context}
'''
        prompt = ChatPromptTemplate.from_template(template)
        model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, api_key=OPENAI_API_KEY)
        doc_chain = create_stuff_documents_chain(model, prompt)
        chain = create_retrieval_chain(retriever, doc_chain)
        result = chain.invoke({"input": "ì—°ìŠµë¬¸ì œ ìƒì„±"})

        try:
            exercises = json.loads(result['answer'])
        except:
            return "ì—°ìŠµë¬¸ì œ ìƒì„± ì‹¤íŒ¨: JSON ë³€í™˜ ë¶ˆê°€", []

        current_questions = [ex['question'] for ex in exercises]
        current_answers = [ex['answer'] for ex in exercises]
        return "\n\n".join(current_questions), ""  # ë¬¸ì œë§Œ ë¨¼ì € í‘œì‹œ
    except Exception as e:
        return f"ì—°ìŠµë¬¸ì œ ìƒì„± ì˜¤ë¥˜: {str(e)}", ""

# ----------------------------
# ì •ë‹µ ê³µê°œ
# ----------------------------
def show_answer(index):
    if 0 <= index < len(current_answers):
        return current_answers[index]
    return "ì •ë‹µ ì—†ìŒ"

# ----------------------------
# ì§ˆë¬¸ ë‹µë³€ (PDF ê¸°ë°˜)
# ----------------------------
def answer_question(pdf_file, question):
    try:
        vectorstore = load_pdf_to_vector_store(pdf_file)
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})

        template = '''ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.

<ë¬¸ë§¥>
{context}
</ë¬¸ë§¥>

ì§ˆë¬¸: {input}

ë‹µë³€:'''
        prompt = ChatPromptTemplate.from_template(template)
        model = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, api_key=OPENAI_API_KEY)
        doc_chain = create_stuff_documents_chain(model, prompt)
        chain = create_retrieval_chain(retriever, doc_chain)
        response = chain.invoke({'input': question})
        return response['answer']
    except Exception as e:
        return f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ----------------------------
# Gradio ì¸í„°í˜ì´ìŠ¤
# ----------------------------
def create_interface():
    with gr.Blocks(title="í•™ìŠµ PDF QA & ìš”ì•½ ì‹œìŠ¤í…œ") as demo:
        gr.Markdown("# í•™ìŠµ PDF QA & ìš”ì•½ ì‹œìŠ¤í…œ")
        gr.Markdown("PDF ì—…ë¡œë“œ â†’ ìš”ì•½, ì—°ìŠµë¬¸ì œ ìƒì„±, ì§ˆë¬¸ ë‹µë³€ ê¸°ëŠ¥ ì œê³µ")

        with gr.Row():
            with gr.Column(scale=1):
                pdf_input = PDF(label="PDF ì—…ë¡œë“œ")

                summarize_btn = gr.Button("ğŸ“„ PDF ìš”ì•½")
                exercise_btn = gr.Button("ğŸ“ ì—°ìŠµë¬¸ì œ ìƒì„±")

            with gr.Column(scale=2):
                chatbot = gr.Chatbot(label="ğŸ’¬ ì§ˆë¬¸")
                msg = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸...")

                submit_btn = gr.Button("ğŸ“¤ ì§ˆë¬¸í•˜ê¸°")
                clear_btn = gr.Button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”")

                # ì—°ìŠµë¬¸ì œ ì •ë‹µ ê³µê°œ
                answer_index = gr.Number(label="ì •ë‹µ ê³µê°œ ë²ˆí˜¸", value=0, precision=0)
                show_answer_btn = gr.Button("ì •ë‹µ í™•ì¸")
                answer_output = gr.Textbox(label="ì •ë‹µ")

        # PDF ìš”ì•½ ë²„íŠ¼
        summarize_btn.click(lambda pdf_file: [(pdf_file, summarize_pdf(pdf_file))], pdf_input, chatbot)
        # ì—°ìŠµë¬¸ì œ ë²„íŠ¼
        exercise_btn.click(generate_exercises, pdf_input, [chatbot, answer_output])
        # ì§ˆë¬¸ ë²„íŠ¼
        def respond(message, chat_history, pdf_file):
            if not message.strip():
                return chat_history, ""
            answer = answer_question(pdf_file, message)
            chat_history.append((message, answer))
            return chat_history, ""
        submit_btn.click(respond, [msg, chatbot, pdf_input], [chatbot, msg])
        msg.submit(respond, [msg, chatbot, pdf_input], [chatbot, msg])
        clear_btn.click(lambda: ([], ""), outputs=[chatbot, msg])

        # ì •ë‹µ ê³µê°œ ë²„íŠ¼
        show_answer_btn.click(lambda i: show_answer(int(i)), answer_index, answer_output)

    return demo

# ----------------------------
# ì‹¤í–‰
# ----------------------------
if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        share=False,
        debug=True,
        server_name="127.0.0.1",
        server_port=7860
    )
