import os
import re
import hashlib
import json
import logging
from typing import List, Tuple, Optional, Dict, Any
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_tavily import TavilySearch
import pymysql

# ==========================================================
# í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", "3306"))
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USERNAME")
DB_PASSWORD = os.getenv("DB_PASSWORD")

if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY ë¯¸ì„¤ì •!")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
tavily_tool = TavilySearch(api_key=TAVILY_API_KEY, max_results=3) if TAVILY_API_KEY else None

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

UPLOAD_DIR = "uploaded_pdfs"
VECTOR_DIR = "vector_stores"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(VECTOR_DIR, exist_ok=True)

# ==========================================================
# Pydantic ëª¨ë¸ ì •ì˜
# ==========================================================
class PdfPathsRequest(BaseModel):
    pdf_paths: List[str]

class ChapterRequest(BaseModel):
    pdf_paths: Optional[List[str]] = None
    chapter_request: Optional[str] = None

class QuizGenerationRequest(BaseModel):
    pdf_paths: List[str]
    num_questions: int = Field(5, ge=1, le=20)
    difficulty: str = Field("MEDIUM", pattern="^(EASY|MEDIUM|HARD)$")

# ==========================================================
# PDF â†’ í…ìŠ¤íŠ¸ ë° ë²¡í„°í™”
# ==========================================================
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        return "\n".join([p.page_content for p in pages])
    except Exception as e:
        logger.error(f"PDF ì¶”ì¶œ ì‹¤íŒ¨ ({pdf_path}): {e}")
        return ""

def get_or_create_vectorstore(pdf_path: str) -> Optional[FAISS]:
    normalized_path = pdf_path.replace("\\", "/")
    path_hash = hashlib.md5(normalized_path.encode()).hexdigest()
    vector_path = os.path.join(VECTOR_DIR, path_hash)
    if os.path.exists(vector_path):
        return FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)
    try:
        loader = PyPDFLoader(pdf_path)
        texts = [p.page_content for p in loader.load()]
        vs = FAISS.from_texts(texts, embeddings)
        vs.save_local(vector_path)
        logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {pdf_path}")
        return vs
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ìƒì„± ì‹¤íŒ¨ ({pdf_path}): {e}")
        return None

def combine_vectorstores(pdf_paths: List[str]) -> Tuple[Optional[FAISS], Optional[str]]:
    vectorstores = []
    combined_content = ""
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            continue
        vs = get_or_create_vectorstore(pdf_path)
        if vs:
            vectorstores.append(vs)
            combined_content += extract_text_from_pdf(pdf_path) + "\n\n--- PDF ë¶„ë¦¬ ---\n\n"
    if not vectorstores:
        return None, None
    main_vs = vectorstores[0]
    for i in range(1, len(vectorstores)):
        main_vs.merge_from(vectorstores[i])
    return main_vs, combined_content.strip()

# ==========================================================
# ì±•í„° ê°ì§€ ë¡œì§
# ==========================================================
def detect_chapters_by_regex(text: str) -> List[str]:
    patterns = [
        r"ì œ\s*\d+\s*ì¥",
        r"CHAPTER\s+\d+",
        r"Chapter\s+\d+",
        r"\b\d+\.\d+",
        r"Section\s+\d+",
        r"Part\s+[IVXLC\d]+"
    ]
    found = set()
    for p in patterns:
        matches = re.findall(p, text)
        for m in matches:
            found.add(m.strip())
    return sorted(found)

def detect_chapters_by_llm(text: str) -> List[str]:
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì±•í„° ì œëª©ì„ ìˆœì„œëŒ€ë¡œ ë½‘ì•„ì£¼ì„¸ìš”.
ì˜ˆì‹œ ì¶œë ¥: ["ì œ1ì¥ ê°œìš”", "ì œ2ì¥ ì´ë¡ "]
---
{text[:4000]}
"""
    try:
        response = llm.invoke(prompt)
        lines = re.findall(r"ì œ?\s*\d+\s*ì¥|CHAPTER\s+\d+|Chapter\s+\d+|\d+\.\d+", response.content)
        return sorted(set(lines))
    except Exception as e:
        logger.warning(f"âš ï¸ LLM ì±•í„° ê°ì§€ ì‹¤íŒ¨: {e}")
        return []

def get_total_chapters(text: str) -> Dict[str, Any]:
    regex_chapters = detect_chapters_by_regex(text)
    if regex_chapters:
        return {"method": "regex", "total_chapters": len(regex_chapters), "chapter_list": regex_chapters}
    llm_chapters = detect_chapters_by_llm(text)
    if llm_chapters:
        return {"method": "llm", "total_chapters": len(llm_chapters), "chapter_list": llm_chapters}
    return {"method": "none", "total_chapters": 1, "chapter_list": []}

# ==========================================================
# DBì—ì„œ content_idë¡œ PDF ê²½ë¡œ ì¡°íšŒ
# ==========================================================
def get_pdf_paths_for_content(content_id: int):
    connection = None
    paths = []
    try:
        connection = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_DATABASE,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )
        with connection.cursor() as cursor:
            sql = "SELECT file_path FROM contents WHERE id=%s"
            cursor.execute(sql, (content_id,))
            result = cursor.fetchall()
            paths = [row['file_path'] for row in result if row['file_path']]
    except Exception as e:
        logger.error(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if connection:
            connection.close()
    return paths

# ==========================================================
# FastAPI ì´ˆê¸°í™”
# ==========================================================
app = FastAPI(title="PDF í•™ìŠµ ë„ìš°ë¯¸ API (Full í†µí•©)")

# ==========================================================
# âœ… /health
# ==========================================================
@app.get("/health/")
async def health_check():
    return {"status": "ok"}

# ==========================================================
# âœ… /upload_pdfs
# ==========================================================
@app.post("/upload_pdfs/")
async def upload_pdfs(files: List[UploadFile] = File(...)):
    if not files:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    logger.info("ğŸ“¥ ì—…ë¡œë“œ ìš”ì²­ ìˆ˜ì‹  | files=%d", len(files))

    saved_files: List[str] = []
    created_vectors_for: List[str] = []
    combined_text_parts: List[str] = []

    for file in files:
        save_path = os.path.join(UPLOAD_DIR, file.filename)
        try:
            # 1) íŒŒì¼ ì €ì¥
            with open(save_path, "wb") as f:
                data = await file.read()
                f.write(data)
            saved_files.append(save_path)
            logger.info("ğŸ—‚ï¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ | path=%s | size=%d bytes", save_path, len(data))

            # 2) í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì±•í„° ê°ì§€ìš©)
            text = extract_text_from_pdf(save_path)
            if not text.strip():
                logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ë¬¸ì„œ | path=%s", save_path)
            else:
                combined_text_parts.append(text)

            # 3) ë²¡í„° ìƒì„±/ë¡œë“œ
            vs = get_or_create_vectorstore(save_path)
            if vs:
                created_vectors_for.append(save_path)
                logger.info("ğŸ§  ë²¡í„° ìƒì„±/ë¡œë“œ ì™„ë£Œ | path=%s", save_path)
            else:
                logger.warning("âš ï¸ ë²¡í„° ìƒì„± ì‹¤íŒ¨ | path=%s", save_path)

        except Exception as e:
            logger.error("âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ | file=%s | error=%s", file.filename, e)
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    # 4) ì „ì²´ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ ì±•í„° ê°ì§€ (ì—¬ëŸ¬ íŒŒì¼ì´ë©´ í•©ì³ì„œ ê°ì§€)
    if not combined_text_parts:
        raise HTTPException(status_code=400, detail="PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    combined_text = "\n\n--- PDF ë¶„ë¦¬ ---\n\n".join(combined_text_parts)
    detection = get_total_chapters(combined_text)

    total_chapters_result: int = int(detection.get("total_chapters", 1))
    detected_method: str = str(detection.get("method", "none"))
    all_chapters: List[str] = list(detection.get("chapter_list", []))

    logger.info(
        "âœ… ì—…ë¡œë“œ/ë¶„ì„ ì™„ë£Œ | files=%d | vectors=%d | total_chapters=%d | method=%s",
        len(saved_files), len(created_vectors_for), total_chapters_result, detected_method
    )

    return {
        "pdf_paths": saved_files,
        "total_chapters": total_chapters_result,
        "method": detected_method,
        "chapter_list": all_chapters
    }
# ==========================================================
# âœ… /summarize (ì „ì²´ ìš”ì•½)
# ==========================================================
@app.post("/api/contents/{content_id}/summarize")
async def summarize_full(content_id: int):
    pdf_paths = get_pdf_paths_for_content(content_id)
    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œ ì—†ìŒ")
    _, combined_content = combine_vectorstores(pdf_paths)
    if not combined_content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨")
    prompt = f"""
ë‹¤ìŒ PDF ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
í•µì‹¬ ìœ„ì£¼, 5ë¬¸ë‹¨ ì´í•˜.
---
{combined_content[:10000]}
"""
    result = llm.invoke(prompt)
    return {"content_id": content_id, "summaryText": result.content.strip()}

# ==========================================================
# âœ… /summaries (ë‹¨ì›ë³„ ìš”ì•½)
# ==========================================================
@app.post("/api/contents/{content_id}/summaries")
async def summarize_chapter(content_id: int, request: Optional[ChapterRequest] = None):
    """
    pdf_pathsë¥¼ ì•ˆ ë³´ë‚¸ ê²½ìš°, DBì—ì„œ ìë™ìœ¼ë¡œ í•´ë‹¹ content_idì˜ PDF ê²½ë¡œ ì¡°íšŒ
    """
    # 1ï¸âƒ£ pdf_paths ìœ íš¨ì„± í™•ì¸
    pdf_paths = []
    if request and request.pdf_paths:
        pdf_paths = request.pdf_paths
    else:
        pdf_paths = get_pdf_paths_for_content(content_id)  # âœ… DBì—ì„œ ê°€ì ¸ì˜¤ê¸°

    if not pdf_paths:
        raise HTTPException(status_code=404, detail="PDF ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 2ï¸âƒ£ chapter_request ì½ê¸° (ì—†ìœ¼ë©´ ì „ì²´ ìš”ì•½)
    chapter_req = None
    if request and request.chapter_request:
        chapter_req = request.chapter_request

    # 3ï¸âƒ£ PDF ë‚´ìš© ë³‘í•©
    _, combined_content = combine_vectorstores(pdf_paths)
    if not combined_content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨")

    # 4ï¸âƒ£ ì±•í„°ë³„ ìš”ì•½ ìƒì„± (ì´ì „ ë¡œì§ ë™ì¼)
    pattern = r"^(\d+\.\d+(\.\d+)?)\s*(.*)$"
    chapters = {}
    current_chapter = None
    for line in combined_content.splitlines():
        line = line.strip()
        if not line:
            continue
        match = re.match(pattern, line)
        if match:
            current_chapter = match.group(1)
            chapters[current_chapter] = match.group(3) + "\n"
        elif current_chapter:
            chapters[current_chapter] += line + "\n"

    summaries = []
    if chapter_req:
        matched = [k for k in chapters.keys() if k.startswith(chapter_req)]
        combined = "\n".join([chapters[k] for k in matched])
        result = llm.invoke(f"ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\n{combined[:8000]}")
        summaries.append({"chapter": chapter_req, "summaryText": result.content.strip()})
    else:
        for k, v in chapters.items():
            result = llm.invoke(f"{k} ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜:\n{v[:8000]}")
            summaries.append({"chapter": k, "summaryText": result.content.strip()})

    return {"summaries": summaries}

# ==========================================================
# âœ… /ask (ì§ˆì˜ì‘ë‹µ)
# ==========================================================
@app.post("/api/contents/{content_id}/ask")
async def ask_question(content_id: int, question: str = Form(...)):
    pdf_paths = get_pdf_paths_for_content(content_id)
    vectorstore, _ = combine_vectorstores(pdf_paths)
    if vectorstore:
        retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
        rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
        resp = rag_chain.invoke({"query": question})
        if isinstance(resp, dict):
            answer = resp.get("result") or resp.get("output_text")
        else:
            answer = str(resp)
        if answer:
            return {"source": "PDF", "answer": answer.strip()}
    if tavily_tool:
        web_resp = tavily_tool.invoke({"query": question})
        results = web_resp.get("results", [])
        if results:
            top = results[0]
            summary = llm.invoke(f"ì•„ë˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ì‘ì„±:\n{top.get('content','')[:2000]}\nQ:{question}")
            return {"source": "WEB", "answer": summary.content.strip()}
    return {"source": "NONE", "answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

# ==========================================================
# âœ… /quiz/generate (í€´ì¦ˆ ìƒì„±)
# ==========================================================
@app.post("/api/contents/{content_id}/quiz/generate")
async def quiz_generate(content_id: int, request: QuizGenerationRequest):
    pdf_paths = get_pdf_paths_for_content(content_id)
    _, content = combine_vectorstores(pdf_paths)
    if not content:
        raise HTTPException(status_code=400, detail="PDF ë‚´ìš© ì—†ìŒ")
    prompt = f"""
ë‹¹ì‹ ì€ êµì¬ ê¸°ë°˜ ê°ê´€ì‹ ë¬¸ì œ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ {request.num_questions}ê°œì˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.
ë‚œì´ë„: {request.difficulty}
í˜•ì‹(JSON):
{{
"questions":[{{"question":"...","options":["A","B","C","D"],"correct_answer":"ì •ë‹µ","explanation":"ì´ìœ "}}]
}}
ë‚´ìš©:
{content[:12000]}
"""
    result = llm.invoke(prompt)
    try:
        cleaned = result.content.replace("```json","").replace("```","").strip()
        data = json.loads(cleaned)
        return data
    except:
        return {"raw_response": result.content}
